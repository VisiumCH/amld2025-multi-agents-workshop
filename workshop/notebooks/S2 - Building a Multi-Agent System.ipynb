{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment and Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "RETRIEVAL_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate database and tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(pickle_filepath: str) -> list[Document]:\n",
    "    \"\"\"Load documents from a pickle file.\"\"\"\n",
    "    with open(pickle_filepath, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def initialize_vector_store(document_chunks: list[Document]) -> Chroma:\n",
    "    \"\"\"Reset the Chroma collection and initialize a vector store using document chunks.\"\"\"\n",
    "    Chroma().reset_collection()\n",
    "    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "    return Chroma.from_documents(documents=document_chunks, embedding=embedding_model)\n",
    "\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "data_file = \"bloomberg_financial_news_1k.pkl\"\n",
    "\n",
    "documents = load_documents(os.path.join(data_dir, data_file))\n",
    "\n",
    "vector_store = initialize_vector_store(documents[:1000])\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": RETRIEVAL_K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieval(retrieval_query: str) -> list[Document]:\n",
    "    \"\"\"Retrieve documents based on a query.\"\"\"\n",
    "    return retriever.invoke(retrieval_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Workflow and state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Orchestator Workflow](../imgs/8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State: shared state between all the nodes\n",
    "\n",
    "messages: the message chain\n",
    "analyses: the reserach analyses\n",
    "\n",
    "Annotated: detail how to treat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class WorkflowState:\n",
    "    \"\"\"Graph state tracking for the financial analysis workflow.\"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages] = field(default_factory=list)\n",
    "    analyses: Annotated[list, operator.add] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constrained output: ask the model to output a particular format, leverages pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchTask(BaseModel):\n",
    "    \"\"\"Task for the financial analysis workflow.\"\"\"\n",
    "\n",
    "    topic: str = Field(description=\"Topic of the research task.\")\n",
    "    description: str = Field(\n",
    "        description=\"Brief description of the task and its objectives.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class OrchestratorDecision(BaseModel):\n",
    "    \"\"\"List of research tasks for the financial analysis workflow.\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"Rationale for the decision and research tasks.\")\n",
    "    in_scope: bool = Field(\n",
    "        description=\"Wether the client request is in scope for the financial analysis.\"\n",
    "    )\n",
    "    research_tasks: list[ResearchTask] | None = Field(\n",
    "        description=\"List of research tasks to be completed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CIA Agent\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "CIA_PROMPT = \"\"\"\n",
    "You are a Client Interface Agent (CIA) in a financial analysis system. You have multiple Research Agents with access to Bloomberg Financial News under your supervision.\n",
    "\n",
    "Given a client request, provide a concise, polite and professional response regarding the feasibility of the request and the approach that will be taken to address it.\n",
    "\n",
    "If the user's request is addressable, create a short list of highly specific research topics that the Research Agents will investigate to fulfill the client's request.\n",
    "\"\"\"\n",
    "\n",
    "cia_model = base_model.with_structured_output(OrchestratorDecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cia(state: WorkflowState) -> Command[Literal[\"research\", END]]:\n",
    "    \"\"\"Orchestrator that generates a plan for the report.\"\"\"\n",
    "    orchestrator_output = cia_model.invoke(\n",
    "        [\n",
    "            SystemMessage(CIA_PROMPT),\n",
    "            *state.messages,\n",
    "        ]\n",
    "    )\n",
    "    print(orchestrator_output)\n",
    "\n",
    "    return Command(\n",
    "        update={\"messages\": orchestrator_output.response},\n",
    "        goto=[Send(\"research\", task) for task in orchestrator_output.research_tasks]\n",
    "        if orchestrator_output.in_scope\n",
    "        else END,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRA_PROMPT = \"\"\"\n",
    "You are a Research Agent in a financial analysis system. You are tasked with writing a concise research report on a specific topic provided by the Client Interface Agent (CIA) based on available documents.\n",
    "\n",
    "To do so, you have access to a Bloomberg Financial News database that you can query. You should query the vector store for documents relevant to your task and write a concise summary of the information you find.\n",
    "\n",
    "Your report should be short and informative, conveying only the most important information from the documents, to allow a Synthesis Agent to quickly generate a report for the client based on the findings of all Research Agents.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tools = [retrieval]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "bra_model = base_model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bra(task: ResearchTask) -> Command[Literal[\"synthesizer\"]]:\n",
    "    \"\"\"Research agent that can query the vector store for relevant documents.\"\"\"\n",
    "    research = bra_model.invoke(\n",
    "        [\n",
    "            SystemMessage(BRA_PROMPT),\n",
    "            HumanMessage(\n",
    "                f\"Research Task: {task.topic}\\n\\n Description: {task.description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if research.tool_calls:\n",
    "        tool_call = research.tool_calls[0]\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        documents = tool.invoke(tool_call[\"args\"])\n",
    "\n",
    "        print([doc.metadata[\"Headline\"] for doc in documents])\n",
    "\n",
    "        research = base_model.invoke(\n",
    "            [\n",
    "                SystemMessage(BRA_PROMPT),\n",
    "                HumanMessage(\n",
    "                    f\"Research Task: {task.topic}\\n\\n Description: {task.description}\"\n",
    "                ),\n",
    "                research,\n",
    "                ToolMessage(\n",
    "                    content=\"\\n\\n\".join(\n",
    "                        [\n",
    "                            f\"{doc.metadata['Headline']}\\n{doc.page_content}\"\n",
    "                            for doc in documents\n",
    "                        ]\n",
    "                    ),\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    print(research.content)\n",
    "\n",
    "    return Command(\n",
    "        update={\"analyses\": [research.content]},\n",
    "        goto=\"synthesizer\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSSA_PROMPT = \"\"\"\n",
    "You are a Market Strategist & Synthesis Agent (MSSA) in a financial analysis system. You receive the analysis reports generated by the Research Agents, and are tasked with synthesizing the information into a final report for the client.\n",
    "\n",
    "You should cross-reference the analyses and write a final report that covers the key findings in a clear and concise manner to address the client's request.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mssa(state: WorkflowState) -> Command[Literal[END]]:\n",
    "    \"\"\"Synthesize full report from research analyses.\"\"\"\n",
    "    analyses_reports = state.analyses\n",
    "\n",
    "    complete_analyses = \"\\n\\n---\\n\\n\".join(analyses_reports)\n",
    "\n",
    "    synthesizer_output = base_model.invoke(\n",
    "        [\n",
    "            SystemMessage(MSSA_PROMPT),\n",
    "            *state.messages,\n",
    "            HumanMessage(complete_analyses),\n",
    "        ]\n",
    "    )\n",
    "    print(synthesizer_output.content)\n",
    "\n",
    "    return Command(\n",
    "        update={\"messages\": synthesizer_output},\n",
    "        goto=END,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build workflow\n",
    "graph_builder = StateGraph(WorkflowState)\n",
    "\n",
    "# Entry point\n",
    "graph_builder.set_entry_point(\"orchestrator\")\n",
    "\n",
    "# Add the nodes\n",
    "graph_builder.add_node(\"orchestrator\", cia)\n",
    "graph_builder.add_node(\"research\", bra)\n",
    "graph_builder.add_node(\"synthesizer\", mssa)\n",
    "\n",
    "# The edges are defined by the commands\n",
    "\n",
    "# Compile the workflow\n",
    "app = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "final_state = app.invoke(\n",
    "    {\n",
    "        \"messages\": \"I want to invest in the technology sector. Can you please define an investment strategy?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(final_state.analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(final_state[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
