{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first part of our workshop! In this session, we'll explore how to build AI-powered applications using **LangChain**, a popular framework for developing applications with Large Language Models (LLMs). We'll start with a simple chatbot and then enhance it with Retrieval Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up our environment. We'll use OpenAI's models, so we need an API key. You can define your `OPENAI_API_KEY` in the `.env` file.\n",
    "\n",
    "The code retrieve the key and sets some global configurations:\n",
    "- `LLM_MODEL`: The specific model we'll use\n",
    "- `LLM_TEMPERATURE`: Controls randomness in responses (0 means very deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_TEMPERATURE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with creating a basic chatbot using **LangChain**. We'll use:\n",
    "- `ChatOpenAI`: The interface to OpenAI's chat models\n",
    "- `SystemMessage`: Defines the bot's behavior and role\n",
    "- `HumanMessage`: Represents user input\n",
    "\n",
    "Our chatbot will act as a Financial Analyst. We'll create it by:\n",
    "1. Instantiating the model\n",
    "2. Defining a system prompt that sets the bot's role\n",
    "3. Sending a user query and getting a response with `.invoke()`\n",
    "\n",
    "This demonstrates the basic pattern of LLM interactions: prompt â†’ response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = \"\"\"\n",
    "You are a Financial Analyst. Do your best to help the client with their request based on your expertise. Give a succinct and clear response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"I want to invest in the tech sector. What are the best options?\"\n",
    "\n",
    "response = base_model.invoke(\n",
    "    [\n",
    "        SystemMessage(BASE_PROMPT),\n",
    "        HumanMessage(request),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the exciting part! RAG is a technique that enhances LLM responses by giving them access to external knowledge. Instead of relying solely on the model's training data, we can provide relevant information from our own database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement RAG, we need:\n",
    "1. A collection of documents (in our case, a currated set of 1'000 articles from Bloomberg financial news)\n",
    "2. A way to search these documents efficiently (vector database)\n",
    "3. A method to retrieve relevant information based on queries\n",
    "\n",
    "Here we use:\n",
    "- `Chroma`: A vector database for storing and retrieving documents\n",
    "- `OpenAIEmbeddings`: Converts text into vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first set up the global configuration for our retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "RETRIEVAL_K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then define helper methods to load our documents and store them in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(pickle_filepath: str) -> list[Document]:\n",
    "    \"\"\"Load documents from a pickle file.\"\"\"\n",
    "    with open(pickle_filepath, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def initialize_vector_store(document_chunks: list[Document]) -> Chroma:\n",
    "    \"\"\"Reset the Chroma collection and initialize a vector store using document chunks.\"\"\"\n",
    "    Chroma().reset_collection()\n",
    "    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "    return Chroma.from_documents(documents=document_chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our documents and inspect the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "data_file = \"bloomberg_financial_news_1k.pkl\"\n",
    "\n",
    "documents = load_documents(os.path.join(data_dir, data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_str = f\"{documents[0].metadata['Headline']}\\n\\n{documents[0].page_content}\"\n",
    "Markdown(doc_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Vector Store and the Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector store and retriever are key components of our RAG system. Here's what happens in this section:\n",
    "\n",
    "- Initialize a new Chroma vector store with these documents\n",
    "- Create a retriever that will fetch the `RETRIEVAL_K` most relevant documents according to their embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = initialize_vector_store(documents)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": RETRIEVAL_K})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retriever acts like a smart search engine - when given a question or topic, it returns the most relevant documents from our database. It does so by finding the documents similar embeddings to the query. In LangChain, this is also done with `.invoke()`. Let's try an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"tech sector market trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now augment our basic chatbot by providing it access to the retriever using **LangChain** tools, which allow the model to:\n",
    "- Query the document database if needed\n",
    "- Provide an answer based on the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a tool with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a tool using the `@tool` decorator from **LangChain** and provide it to the model using `.bind_tools()`. The model will receive all the relevant information about the tool thanks to the decorator. This way it knows how it works and can decide when to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieval(retrieval_query: str) -> list[Document]:\n",
    "    \"\"\"Retrieve documents based on a query.\"\"\"\n",
    "    return retriever.invoke(retrieval_query)\n",
    "\n",
    "\n",
    "tools = [retrieval]\n",
    "tools_by_name = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT = \"\"\"\n",
    "You are a Financial Analyst with access to a Bloomberg Financial News database.\n",
    "\n",
    "Query the database to help the client with their request. Give a succinct and clear response based on the information you find.\n",
    "\"\"\"\n",
    "\n",
    "rag_model = base_model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"I want to invest in the tech sector. What are the best options?\"\n",
    "\n",
    "rag_response = rag_model.invoke(\n",
    "    [\n",
    "        SystemMessage(RAG_PROMPT),\n",
    "        HumanMessage(request),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the answer. As we can see its content is empty, but a tool call has been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content: {rag_response.content}\\n\\nTool Calls: {rag_response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the retrieval tool to retrieve documents following the model's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_response.tool_calls:\n",
    "    tool_call = rag_response.tool_calls[0]\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    documents = tool.invoke(tool_call[\"args\"])\n",
    "\n",
    "    documents_str = \"\\n\\n\".join(\n",
    "        [f\"{doc.metadata['Headline']}\\n\\n{doc.page_content}\\n\" for doc in documents]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(documents_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the tool's output to the message chain with `ToolMessage`, so the model can answer based on the retrieved documents.\n",
    "\n",
    "*Note: Here we use the base model instead of the RAG model to limit our agent to one retrieval call. A fully autonomous agent could decide to make subsequent calls to best answer the request*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = base_model.invoke(\n",
    "    [\n",
    "        SystemMessage(RAG_PROMPT),\n",
    "        HumanMessage(request),\n",
    "        rag_response,\n",
    "        ToolMessage(content=documents_str, tool_call_id=tool_call[\"id\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Watch the temperature setting: Lower values (like 0) are usually better for factual responses\n",
    "- Pay attention to the number of retrieved documents (`RETRIEVAL_K`): More isn't always better\n",
    "- The system prompt is crucial: It sets the context and behavior of your bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just learned how to create a chatbot and augment it with a retrieval tool using **LangChain**, this concludes the first part of our workshop!\n",
    "\n",
    "In the next section, we'll discover **LangGraph** and show how it allows to build sophisticated and flexible LLMs workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
